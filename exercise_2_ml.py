# -*- coding: utf-8 -*-
"""Exercise 2 ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sHqvq81JodQiRJOpxq4B_LZ_PQoFRD13

# loading libraries
"""

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as plt
import seaborn as sns

df=pd.read_csv("water_potability.csv")

"""# 1. Display the first 5 rows of the table:

"""

print("5 first rows of the table:")
display(df.head())

"""# Description of the data:

"""

display(df.describe())

"""# Checking missing values per attribute:"""

missing=df.isnull().sum()
display(missing)

"""# Histograms and each feature:"""

import matplotlib.pyplot as plt

df.hist(bins=20, figsize=(15,10), edgecolor='black')
plt.suptitle("Histograms of each feature", fontsize=16)
plt.show()

"""# Bar chart for the Potability feature:"""

import matplotlib.pyplot as plt
import seaborn as sns

sns.countplot(x='Potability', data=df )
plt.show()

"""# Percentages of drinkable and undrinkable water:"""

potability_counts=df['Potability'].value_counts(normalize=True)*100
print(potability_counts.round(2))

"""# Evaluation of the data:
Based on the description of the data, I can say that the data is of moderate quality. In particular, for the pH value, I see that the average value is close to 7, which is natural for water. In addition, I see that the hardness and total solids take on many values, i.e., they have a wide range of values, which indicates that the data came from many different water sources, which I would characterize as good.

Of course, there are gaps of 20%-25% in some characteristics such as pH, sulfates, and trihalomethanes. This cannot be good because the model will not be trained correctly if there are gaps in the data.

I also note that most variables have an asymmetrical distribution and some show extreme values, such as organic carbon and solids.

# 2. Data separation by category of the **pH**
"""

low_ph=df[df['ph']<6.5]
normal_ph=df[(df['ph']>=6.5) & (df['ph']<=8.5)]
high_ph=df[df['ph']>8.5]

"""# Calculation of drinkable and non-drinkable water percentages:"""

def potability_percent(sub_df):
    return sub_df['Potability'].value_counts(normalize=True) * 100


print(" pH < 6.5")
print(potability_percent(low_ph))

print("\n 6.5 ≤ pH ≤ 8.5")
print(potability_percent(normal_ph))

print("\n pH > 8.5")
print(potability_percent(high_ph))

"""# Evaluation:

From the results of the percentages, we observe that the highest percentage of drinkable water from our data is indeed within the range of 6.5-8.5 (43.9%). However, within this range, there is also a significant amount of non-potable water, at 56.1%. We can see, therefore, that it is not 100% potable. Furthermore, there is a lot of drinkable water outside the range of 6.5-8.5. Therefore, this data does not fully correspond to my data.

# 3. Scatter plot creation
"""

import matplotlib.pyplot as plt

plt.figure(figsize=(8,6))
colors = df['Potability'].map({0: 'red', 1: 'blue'})
plt.scatter(df['ph'], df['Chloramines'], c=colors, alpha=0.6)
plt.title("Διάγραμμα pH vs Chloramines")
plt.xlabel("pH")
plt.ylabel("Chloramines (ppm)")
plt.show()

"""# Evaluation:
I see that the points on the graph are very scattered and we cannot draw conclusions based on these two variables alone. There are drinkable and undrinkable samples across the entire range of pH and chloramines, and the 4 ppm line does not even separate the types.
For better classification, other characteristics will be needed; these two alone are not enough.

# 4. Filling in gaps in the data
"""

for col in df.columns:
    if df[col].isnull().sum() > 0:
       mean_val=df[col].mean()
       df[col]= df[col].fillna(mean_val)

"""# 5. Numpy arrays creation:"""

from re import X
import numpy as np
X=df.drop("Potability", axis=1).values
y=df["Potability"].values

"""Separation in train-test:"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=0)

print("train test:", X_train.shape, y_train.shape)
print("test test:", X_test.shape, y_test.shape)

"""# 6. Desicion Tree

Creation of the model and training in the training set:
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

dt=DecisionTreeClassifier(random_state=0)
dt.fit(X_train, y_train)

"""Prediction and accuracy measurement"""

y_train_pred=dt.predict(X_train)
y_test_pred=dt.predict(X_test)

train_acc=accuracy_score(y_train, y_train_pred)
test_acc=accuracy_score(y_test, y_test_pred)

print("Accuracy to Training set:", train_acc)
print("Accuracy to test set:", test_acc)

"""# 7. Parameter Selection


"""

from logging import critical
import itertools

criteria=["gini", "entropy"]
max_dephts=[None, 3, 5]
min_samples_splits=[2, 5]
min_samples_leafs=[1,2]
max_features_list=[None, "sqrt"]
ccp_alphas=[0, 0.01]

results=[]

"""Testing all of the conbinations:"""

for crit, depth, split, leaf, feat, ccp in itertools.product(criteria, max_dephts, min_samples_splits, min_samples_leafs, max_features_list, ccp_alphas):
  dt=DecisionTreeClassifier(criterion=crit, max_depth=depth, min_samples_split=split, min_samples_leaf=leaf, max_features=feat, ccp_alpha=ccp, random_state=0)

  dt.fit(X_train, y_train)

  train_acc=accuracy_score(y_train, dt.predict(X_train))
  test_acc=accuracy_score(y_test, dt.predict(X_test))

  results.append([crit, depth, split, leaf, feat, ccp, train_acc, test_acc])

"""Data Frame creation with all of the results:"""

results_df=pd.DataFrame(results, columns=["criterion", "max_depth", "min_samples_split",
                                          "min_samples_leaf", "max_features", "ccp_alpha", "train_cc", "test_acc"])

results_df.sort_values(by="test_acc", ascending=False).head(10)

"""# 8.Filter with max depth 3:"""

from sklearn.tree import DecisionTreeClassifier, plot_tree
depth3_df = results_df[results_df['max_depth'] == 3]

"""Best Selection:"""

best_depth3 = depth3_df.loc[depth3_df['test_acc'].idxmax()]
print("Best combination wit max depth 3")
print(best_depth3)
print()

"""Re-train the tree:"""

best_dt3 = DecisionTreeClassifier(
    criterion=best_depth3['criterion'],
    max_depth=3,
    min_samples_split=int(best_depth3['min_samples_split']),
    min_samples_leaf=int(best_depth3['min_samples_leaf']),
    max_features=None if best_depth3['max_features'] == 'None' else best_depth3['max_features'],
    ccp_alpha=float(best_depth3['ccp_alpha']),
    random_state=0)
best_dt3.fit(X_train, y_train)

"""Visualization of the tree:"""

plt.figure(figsize=(22,10))
plot_tree(
    best_dt3,
    feature_names=df.drop('Potability', axis=1).columns,
    class_names=['No drinkable', 'Drinkable'],
    filled=True,
    rounded=True,
    fontsize=10
)
plt.title("The tree with max_depth=3")
plt.show()

"""The decision tree he has created has a depth of 3, meaning that it makes decisions based on 3 characteristics of water.

# 9. Feature Importance:
"""

from sklearn.metrics import accuracy_score

best_idx = results_df['test_acc'].idxmax()
best_row = results_df.loc[best_idx]
print("Best combination")
print(best_row)

"""Re-Training of the tree with the best combination:"""

best_dt = DecisionTreeClassifier(
    criterion=best_row['criterion'],
    max_depth=None if best_row['max_depth'] == 'None' else int(best_row['max_depth']),
    min_samples_split=int(best_row['min_samples_split']),
    min_samples_leaf=int(best_row['min_samples_leaf']),
    max_features=None if best_row['max_features'] == 'None' else best_row['max_features'],
    ccp_alpha=float(best_row['ccp_alpha']),
    random_state=0
)

best_dt.fit(X_train, y_train)

"""Accuracy on the training and test set:"""

train_acc = accuracy_score(y_train, best_dt.predict(X_train))
test_acc = accuracy_score(y_test, best_dt.predict(X_test))
print(f"Train Accuracy: {train_acc:.4f}")
print(f"Test Accuracy: {test_acc:.4f}")

"""Feature importance:"""

feature_names = df.drop("Potability", axis=1).columns
importances = best_dt.feature_importances_

feat_imp_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

print("\nFeature importance:")
display(feat_imp_df)

"""Bar Chart creation:"""

plt.figure(figsize=(10,6))
plt.barh(feat_imp_df['Feature'], feat_imp_df['Importance'], color='skyblue')
plt.gca().invert_yaxis()
plt.title("Feature Importances (Decision Tree)")
plt.xlabel("Importance")
plt.tight_layout()
plt.show()

"""Having created a graph showing the importance of each characteristic, we see that the five most important characteristics are: Sulfates, pH, Chloramines, Hardness and Solids. Therefore, if the device could support up to 5 chemical analyses, it would have to extract these characteristics, as they are the ones that are most important.

# 11. Random Forest vs Decision Tree:

A random forest may be more accurate than a single tree because it combines many decision trees and does not rely solely on one tree, thus reducing errors and making it more reliable and stable. Also, in a random forest, we have a reduction in overfitting because it generalizes better and has less variance, so it can predict better.

# 12. Random Forest
"""

from sklearn.ensemble import RandomForestClassifier

criteria = ['gini', 'entropy']
max_depths = [None, 3, 5]
min_samples_splits = [2, 5]
min_samples_leafs = [1, 2]
max_features_opts = [None, 'sqrt']
n_estimators_opts = [50, 100, 200]

rf_results = pd.DataFrame(columns=[
    'criterion', 'max_depth', 'min_samples_split', 'min_samples_leaf',
    'max_features', 'n_estimators', 'train_acc', 'test_acc'
])

"""Loop for all the combinations:"""

for criterion in criteria:
    for max_depth in max_depths:
        for min_split in min_samples_splits:
            for min_leaf in min_samples_leafs:
                for max_feat in max_features_opts:
                    for n_est in n_estimators_opts:
                        rf = RandomForestClassifier(
                            criterion=criterion,
                            max_depth=max_depth,
                            min_samples_split=min_split,
                            min_samples_leaf=min_leaf,
                            max_features=max_feat,
                            n_estimators=n_est,
                            random_state=0
                        )
                        rf.fit(X_train, y_train)
                        train_acc = accuracy_score(y_train, rf.predict(X_train))
                        test_acc = accuracy_score(y_test, rf.predict(X_test))

                        rf_results = pd.concat([rf_results, pd.DataFrame([{
                            'criterion': criterion,
                            'max_depth': max_depth,
                            'min_samples_split': min_split,
                            'min_samples_leaf': min_leaf,
                            'max_features': max_feat,
                            'n_estimators': n_est,
                            'train_acc': train_acc,
                            'test_acc': test_acc
                        }])], ignore_index=True)

"""Display of the first rows:"""

rf_results.sort_values(by='test_acc', ascending=False).head(10)

"""# 13. Explaination on the importance of the prediction of the model:

In this application, I believe that the model should better predict non-potable water with greater accuracy. Incorrect classification of drinkable and non-drinkable water can have very serious consequences for human health. If the model incorrectly predicts non-drinkable water as drinkable, it will clearly be much worse than the opposite case.

# 14. Best tree or best Random forest

I believe that a random forest is the best legal choice for a company. I believe that a random forest is more reliable and stable in a given situation because it consists of many independent trees and combines many predictions. Furthermore, if a mistake is made and there is an inaccurate prediction of drinkable and undrinkable water, the company can point to the random forest and show that it made its decision based on a scientific method and approach and did not rely on just one tree.
"""